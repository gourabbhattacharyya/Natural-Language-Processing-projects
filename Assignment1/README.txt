System configuration:
--------------------

OS: MAC OS 10.13.3

Python Version: Python 2.7.10


Implmentation:
-------------
	As part the Word2Vec implementation, I have updated and created total of 3 files. Details of them are below:

	1. word2vec_basic.py : 

							This is the main file where the execution starts for training using word embeddings and generate the similar words by means of difference vector.
							In this file I have updated the code for generate_batch method. This function is used to generate the batches and true lebel for the data per batch. Using the window size, the context words will be generated and checked for the window size at each iteration. For each iteration the genearated batch and lebels are stored in buffer and finally when the sliding window reaches the end of the data then the batch and lebel are returned.

							Once the batch data generated the remaining code(provided) use the pretarined model to train further using Cross entropy and NCE(Noise contrastive estimation) loss function and increase the accuracy of the generated model.



	2.loss_func.py : 

							This is the file which contains 2 different implementation of loss function:

							Cross entropy loss : This loss function use the input word embeddings and true lebels of the word vectors to calculate the loss and return the value to the word2vec_basic function.


							Noise contrastive estimation loss : This loss function is configured to get more accuracy of the model to generate similar words. Here we use negative sampling method. We first filter out the word vectors and probability using the true word lebel and get the regression values by means of applying sigmoid function. Then again we apply another filter using negative samples and get the sigmoid value of the same. The final loss will be the negative summation of the 2 calculated sigmoid values.




	3.word_analogy.py : 

							This file is used to generate the predictions using the trained model(cross-entropy and nce) that trained in the previous step. Here, we load the data files with list of words and compare those with the trained data values stored in embeddings format. We get the cosine similarity of the given data and trained data and log them in an output file in the format : 
							<pair1> <pair2> <pair3> <pair4> <least_illustrative_pair> <most_illustrative_pair>





Execution Instruction:
----------------------
						-to execute and train the model using cross-entropy or NCE loss function run below:

							- python word2vec_basic.py cross_entropy  or  python word2vec_basic.py
							- python word2vec_basic.py nce



						- To execute and generate the prediction using the trained model. In the word_analogy.py file select the model by commenting/uncommenting and running below command:

							- python word_analogy.py



						- To generate the accuray of the prediction execute the below command:

							- ./score_maxdiff.pl word_analogy_dev_mturk_answers.txt predictionOutput_cross_entropy.txt result_cross_entropy.txt
							- ./score_maxdiff.pl word_analogy_dev_mturk_answers.txt predictionOutput_nce.txt result_nce.txt


Result FIles:
-------------
			
						- models generated(word2vec_basic.py): These 2 models are generated using the best configurations mentioned below

									- word2vec_cross_entropy.model [for cross-entropy loss function]
									- word2vec_nce.model [for NCE loss function]


						
						- prediction files(output of word_analogy.py):

									- predictionOutput_cross_entropy.txt [This is prediction file for cross-entropy loss using word_analogy_test.txt]
									- predictionOutput_nce.txt [This is prediction file for NCE loss using word_analogy_test.txt]






Best Accuracy(Configuration) Achived for DEV file (word_analogy_dev.txt):
------------------------------------------------------------------------

						- Best accuracy achived for the configuration batch_size = 128, embedding_size = 128, skip_window = 4, num_skips = 8, num_sampled = 64 and with learning rate 1.0



DEV file(word_analogy_dev.txt) accuracy:
---------------------------------------

					- For cross-entropy loss:

						Generated by:                                     score_maxdiff.pl
						Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
						Test File:                                        predictionOutput_cross_entropy.txt
						Number of MaxDiff Questions:                      914
						Number of Least Illustrative Guessed Correctly:   349
						Number of Least Illustrative Guessed Incorrectly: 565
						Accuracy of Least Illustrative Guesses:            38.2%
						Number of Most Illustrative Guessed Correctly:    307
						Number of Most Illustrative Guessed Incorrectly:  607
						Accuracy of Most Illustrative Guesses:             33.6%
						Overall Accuracy:                                  35.9%


					
					- For NCE loss :

						Generated by:                                     score_maxdiff.pl
						Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
						Test File:                                        predictionOutput_nce.txt
						Number of MaxDiff Questions:                      914
						Number of Least Illustrative Guessed Correctly:   351
						Number of Least Illustrative Guessed Incorrectly: 563
						Accuracy of Least Illustrative Guesses:            38.4%
						Number of Most Illustrative Guessed Correctly:    287
						Number of Most Illustrative Guessed Incorrectly:  627
						Accuracy of Most Illustrative Guesses:             31.4%
						Overall Accuracy:                                  34.9%



Report:
-------
					- A PDF report has been generated with all the experimented configuration details and the achived accuracy for the DEV data (word_analogy_dev.txt)
					- This also contains the top 20 similar words according to your NCE and cross entropy model for the words (first, american, would)
					- Finally contains a one page report on summary of NCE loss from the given paper at https://www.cs.toronto.edu/~amnih/papers/wordreps.pdf




																-------------------- END of FILE ----------------------  
